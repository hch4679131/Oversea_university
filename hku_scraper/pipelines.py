import base64
import hashlib
import json
import os
import time
from datetime import datetime
from pathlib import Path

import requests


class SaveJsonPipeline:
    """ä¿å­˜æ–‡ç«  JSONï¼Œç¿»è¯‘æ‘˜è¦åå‘é€åˆ°ä¼ä¸šå¾®ä¿¡ã€‚"""

    def open_spider(self, spider):
        # ç»Ÿä¸€æ•°æ®ç›®å½•ï¼Œä¸çˆ¬è™«ä¿æŒä¸€è‡´ï¼Œé¿å…ç´¢å¼•ä¸ä¸€è‡´å¯¼è‡´é‡å¤å‘é€
        # ä¼˜å…ˆä½¿ç”¨æœåŠ¡å™¨ç›®å½• /root/hku_news_dataï¼›æœ¬åœ°ç¯å¢ƒåˆ™å›é€€åˆ° Desktop/hku_news_data
        server_dir = Path('/root/hku_news_data')
        if server_dir.exists() or os.getenv('SERVER_ENV') == '1':
            self.data_dir = server_dir
            self.data_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.data_dir = (
                Path(os.getenv("USERPROFILE") or os.getenv("HOME") or ".")
                / "Desktop"
                / "hku_news_data"
            )
            self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®çˆ¬è™«åä½¿ç”¨ä¸åŒçš„ç´¢å¼•æ–‡ä»¶
        spider_name = spider.name
        if spider_name == "hku_science_news":
            self.index_file = self.data_dir / "science_news_index.json"
        elif spider_name == "hku_business_news":
            self.index_file = self.data_dir / "business_news_index.json"
        else:  # hku_arts_news æˆ–å…¶ä»–
            self.index_file = self.data_dir / "news_index.json"
        
        if self.index_file.exists():
            try:
                with open(self.index_file, "r", encoding="utf-8") as f:
                    self.index = json.load(f)
            except Exception:
                self.index = {}
        else:
            self.index = {}

    def process_item(self, item, spider):
        # æå–å›¾ç‰‡è·¯å¾„
        images_meta = item.get("images", [])
        local_image_paths = []
        for im in images_meta:
            if "path" in im:
                local_image_paths.append(str(Path(im["path"]).as_posix()))

        # è°ƒç”¨ DeepSeek ç¿»è¯‘å’Œæ‘˜è¦
        zh = self.translate_and_summarize(
            item.get("title") or "", item.get("text") or "", spider
        )

        # æ„å»ºè¾“å‡ºå¯¹è±¡
        out = {
            "title": item.get("title"),
            "url": item.get("url"),
            "text": item.get("text"),
            "images": local_image_paths,
            "scraped_at": item.get("scraped_at"),
            "status": item.get("status", "completed"),
        }

        # æ·»åŠ ç¿»è¯‘å’Œæ‘˜è¦å­—æ®µ
        if zh:
            out.update(
                {
                    "title_zh": zh.get("title_zh"),
                    "summary_zh": zh.get("summary_zh"),
                    "full_text_zh": zh.get("full_text_zh"),
                    "translation_model": zh.get("model"),
                    "translation_at": zh.get("timestamp"),
                }
            )

        # ä¿å­˜ JSON æ–‡ä»¶
        idx = len(self.index) + 1
        filename = f"{idx}_article.json"
        outfile = self.data_dir / filename
        with open(outfile, "w", encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False, indent=2)

        # æ›´æ–°ç´¢å¼•
        url = item.get("url")
        self.index[url] = {
            "title": item.get("title"),
            "file": filename,
            "scraped_at": item.get("scraped_at"),
        }
        with open(self.index_file, "w", encoding="utf-8") as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)

        spider.logger.info(f"[SaveJsonPipeline] saved {outfile}")

        # å‘é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜å’Œæ‘˜è¦ï¼‰
        self.send_to_wechat(out, spider)

        # æ–‡ç« ä¹‹é—´å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        spider.logger.info(
            "[Rate Limit] ç­‰å¾… 40 ç§’åå¤„ç†ä¸‹ä¸€ç¯‡æ–‡ç« ï¼ˆé¿å… API é¢‘ç‡é™åˆ¶ï¼‰..."
        )
        time.sleep(40)

        return item

    def _load_deepseek_key(self):
        """åŠ è½½ DeepSeek API Keyï¼Œä¼˜å…ˆ config/deepseek.jsonï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡"""
        cfg_path = Path(__file__).parent.parent / "config" / "deepseek.json"
        if cfg_path.exists():
            try:
                with open(cfg_path, "r", encoding="utf-8") as f:
                    cfg = json.load(f)
                    if cfg.get("apiKey"):
                        return cfg.get("apiKey")
            except Exception:
                pass
        return os.getenv("DEEPSEEK_API_KEY")

    def translate_and_summarize(self, title, text, spider, retry=3):
        """è°ƒç”¨ DeepSeek ç¿»è¯‘æ ‡é¢˜å¹¶ç”Ÿæˆæ‘˜è¦ï¼Œå¤±è´¥æ—¶é‡è¯•"""
        api_key = self._load_deepseek_key()
        if not api_key:
            spider.logger.warning("[DeepSeek] æœªé…ç½® API Keyï¼Œè·³è¿‡ç¿»è¯‘/æ‘˜è¦")
            return None
        if not text:
            spider.logger.warning("[DeepSeek] æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡ç¿»è¯‘/æ‘˜è¦")
            return None

        for attempt in range(1, retry + 1):
            try:
                if attempt > 1:
                    wait_time = 10 * attempt
                    spider.logger.info(f"[DeepSeek] ç¬¬ {attempt}/{retry} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time} ç§’...")
                    time.sleep(wait_time)
                    
                spider.logger.info(f"[DeepSeek] å¼€å§‹ç¿»è¯‘ (æ ‡é¢˜é•¿åº¦: {len(title)}, æ­£æ–‡é•¿åº¦: {len(text)})")
                
                prompt = (
                    "ä½ æ˜¯ä¸­è‹±æ–‡ç¿»è¯‘å’Œæ–°é—»æ‘˜è¦åŠ©æ‰‹ã€‚\n"
                    "è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
                    "1. æŠŠæ ‡é¢˜ç¿»è¯‘ä¸ºä¸­æ–‡\n"
                    "2. åŸºäºæ­£æ–‡ç”Ÿæˆ 2 å¥å·¦å³çš„ä¸­æ–‡æ¦‚è¿°ï¼ˆæ¯å¥<=120å­—ï¼Œå£å¾„å®¢è§‚ï¼Œé¿å…å¤¸å¼ ï¼‰\n"
                    "3. æŠŠæ•´ç¯‡æ­£æ–‡å®Œæ•´ç¿»è¯‘ä¸ºä¸­æ–‡\n"
                    "ä¸¥æ ¼è¾“å‡º JSONï¼Œå¯¹è±¡åŒ…å« title_zhã€summary_zhã€full_text_zh ä¸‰ä¸ªå­—æ®µã€‚ä¸è¦è¾“å‡ºé¢å¤–æ–‡æœ¬ã€‚\n"
                    f"Title:\n{title}\n\nBody:\n{text}"
                )

                payload = {
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are a concise translator and summarizer. Output only valid JSON.",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    "temperature": 0.3,
                    "max_tokens": 4000,
                }

                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                }

                spider.logger.info("[DeepSeek] æ­£åœ¨è°ƒç”¨ API...")
                resp = requests.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                resp.raise_for_status()
                body = resp.json()
                spider.logger.info(f"[DeepSeek] API å“åº”: {body.get('choices', [{}])[0].get('message', {}).get('content', '')[:100]}")

                content = body["choices"][0]["message"]["content"].strip()
                
                # å¤„ç† DeepSeek å¯èƒ½è¿”å›çš„ markdown ä»£ç å—æ ¼å¼
                if content.startswith("```json"):
                    content = content[7:]  # å»æ‰ ```json
                if content.startswith("```"):
                    content = content[3:]  # å»æ‰ ```
                if content.endswith("```"):
                    content = content[:-3]  # å»æ‰ç»“å°¾çš„ ```
                content = content.strip()
                
                data = json.loads(content)
                model = (
                    body.get("model")
                    or body["choices"][0].get("model")
                    or "deepseek-chat"
                )

                result = {
                    "title_zh": data.get("title_zh"),
                    "summary_zh": data.get("summary_zh"),
                    "full_text_zh": data.get("full_text_zh"),
                    "model": model,
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                }
                spider.logger.info(f"[DeepSeek] ç¿»è¯‘æˆåŠŸ: {result['title_zh']}")
                return result
                
            except requests.exceptions.RequestException as e:
                spider.logger.error(f"[DeepSeek] HTTP è¯·æ±‚å¤±è´¥ (ç¬¬{attempt}æ¬¡): {type(e).__name__}: {e}")
                if attempt == retry:
                    return None
                continue
            except json.JSONDecodeError as e:
                spider.logger.error(f"[DeepSeek] JSON è§£æå¤±è´¥ (ç¬¬{attempt}æ¬¡): {e} (åŸå§‹å“åº”: {content[:200] if 'content' in locals() else 'N/A'})")
                if attempt == retry:
                    return None
                continue
            except Exception as e:
                spider.logger.error(f"[DeepSeek] æœªçŸ¥é”™è¯¯ (ç¬¬{attempt}æ¬¡): {type(e).__name__}: {e}")
                if attempt == retry:
                    return None
                continue
        
        return None

    def send_to_wechat(self, article_data, spider):
        """å‘é€æ–‡ç« åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äººï¼ˆä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜å’Œæ‘˜è¦ï¼‰"""
        try:
            # åŠ è½½ webhook é…ç½®
            webhook_url = None
            config_file = Path(__file__).parent.parent / "config" / "wechat.json"
            if config_file.exists():
                with open(config_file, "r", encoding="utf-8") as f:
                    cfg = json.load(f)
                    webhook_url = cfg.get("webhookUrl")

            if not webhook_url:
                webhook_url = os.getenv("WECHAT_WEBHOOK")

            if not webhook_url:
                spider.logger.info("[WeChat] æœªé…ç½® webhookï¼Œè·³è¿‡å‘é€")
                return

            # ä¼˜å…ˆä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨è‹±æ–‡åŸæ ‡é¢˜
            title = (
                article_data.get("title_zh")
                or article_data.get("title")
                or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
            )
            summary = article_data.get("summary_zh", "")
            full_text = article_data.get("full_text_zh", "")
            url = article_data.get("url", "")
            scraped_at = article_data.get("scraped_at", "")
            
            # è¯†åˆ«æ¥æºå­¦é™¢
            source_label = ""
            if "arts" in spider.name.lower():
                source_label = "ğŸ¨ HKU Arts"
            elif "business" in spider.name.lower():
                source_label = "ğŸ’¼ HKU Business"
            elif "science" in spider.name.lower():
                source_label = "ğŸ”¬ HKU Science"
            
            # å¦‚æœæ²¡æœ‰ä¸­æ–‡ç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡åŸæ–‡ä½œä¸ºé™çº§æ–¹æ¡ˆ
            if not full_text:
                spider.logger.warning("[WeChat] æ— ä¸­æ–‡ç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡åŸæ–‡å‘é€")
                title = article_data.get("title", "ï¼ˆæ— æ ‡é¢˜ï¼‰")
                full_text = article_data.get("text", "")
                if not full_text:
                    spider.logger.warning("[WeChat] æ— ä»»ä½•å†…å®¹å¯å‘é€ï¼Œè·³è¿‡")
                    return

            # å…ˆå‘é€æ¦‚è¿°ï¼ˆå¦‚æœæœ‰ï¼‰
            if summary:
                summary_content = f"**{title}**\n\n{source_label} | ğŸ“ **æ¦‚è¿°**\n{summary.strip()}\n\n[é˜…è¯»åŸæ–‡]({url})\n\n_æŠ“å–æ—¶é—´: {scraped_at}_"
                summary_bytes = len(summary_content.encode("utf-8"))
                spider.logger.info(f"[WeChat] å‘é€æ¦‚è¿°: {summary_bytes} å­—èŠ‚")
                
                payload = {"msgtype": "markdown", "markdown": {"content": summary_content}}
                resp = requests.post(webhook_url, json=payload, timeout=10)
                spider.logger.info(f"[WeChat] æ¦‚è¿°å·²å‘é€: {resp.json()}")
                time.sleep(1)
            else:
                spider.logger.info("[WeChat] æ— æ¦‚è¿°ï¼Œç›´æ¥å‘é€å…¨æ–‡")
            
            # å‘é€å…¨æ–‡ï¼ˆä¸­æ–‡ç¿»è¯‘æˆ–è‹±æ–‡åŸæ–‡ï¼‰
            text = full_text
            content_label = "å…¨æ–‡ç¿»è¯‘" if article_data.get("full_text_zh") else "åŸæ–‡å†…å®¹"

            # æ„å»º Markdown æ¶ˆæ¯
            plain_text = text.replace("\n", " ").strip()
            md_template_prefix = f"**{title}**\n\n{source_label} | ğŸ“„ **{content_label}**\n\n"
            md_template_suffix = f"\n\n[é˜…è¯»åŸæ–‡]({url})\n\n_æŠ“å–æ—¶é—´: {scraped_at}_"
            full_content = md_template_prefix + plain_text + md_template_suffix
            full_bytes = len(full_content.encode("utf-8"))
            max_segment_bytes = 4050

            if full_bytes > max_segment_bytes:
                # æ–‡ç« è¿‡é•¿ï¼Œéœ€è¦åˆ†æ®µå‘é€
                segments = []
                remaining_text = plain_text
                segment_num = 1

                while remaining_text:
                    if segment_num == 1:
                        segment_header = md_template_prefix
                    else:
                        segment_header = (
                            f"**{title}ï¼ˆç»­{segment_num}ï¼‰**\n\n"
                        )

                    header_bytes = len(segment_header.encode("utf-8"))
                    suffix_continue = "\n\n_ï¼ˆå†…å®¹è¾ƒé•¿ï¼Œç»­è§ä¸‹æ¡ï¼‰_"
                    suffix_continue_bytes = len(
                        suffix_continue.encode("utf-8")
                    )
                    suffix_final_bytes = len(
                        md_template_suffix.encode("utf-8")
                    )
                    available_bytes = (
                        max_segment_bytes
                        - header_bytes
                        - suffix_continue_bytes
                    )

                    # äºŒåˆ†æŸ¥æ‰¾æœ€å¤§å¯å®¹çº³å­—ç¬¦æ•°
                    low, high = 0, len(remaining_text)
                    best_chars = 0
                    while low <= high:
                        mid = (low + high) // 2
                        test_segment = remaining_text[:mid]
                        test_bytes = len(test_segment.encode("utf-8"))
                        if test_bytes <= available_bytes:
                            best_chars = mid
                            low = mid + 1
                        else:
                            high = mid - 1

                    segment_text = remaining_text[:best_chars]
                    remaining_after = remaining_text[best_chars:].strip()
                    is_last = len(remaining_after) < 100

                    if is_last:
                        full_segment = (
                            segment_header + segment_text + md_template_suffix
                        )
                        if (
                            len(full_segment.encode("utf-8"))
                            > max_segment_bytes
                        ):
                            available_bytes_final = (
                                max_segment_bytes
                                - header_bytes
                                - suffix_final_bytes
                            )
                            low, high = 0, len(remaining_text)
                            best_chars = 0
                            while low <= high:
                                mid = (low + high) // 2
                                test_segment = remaining_text[:mid]
                                if (
                                    len(test_segment.encode("utf-8"))
                                    <= available_bytes_final
                                ):
                                    best_chars = mid
                                    low = mid + 1
                                else:
                                    high = mid - 1
                            segment_text = remaining_text[:best_chars]
                        segment_md = (
                            segment_header + segment_text + md_template_suffix
                        )
                    else:
                        segment_md = (
                            segment_header + segment_text + suffix_continue
                        )

                    segments.append(segment_md)
                    remaining_text = remaining_text[len(segment_text) :].strip()
                    segment_num += 1

                spider.logger.info(
                    f"[WeChat] æ–‡ç« è¿‡é•¿ï¼Œåˆ†{len(segments)}æ®µå‘é€: åŸæ–‡{full_bytes}å­—èŠ‚"
                )
                for idx, segment_md in enumerate(segments, 1):
                    payload = {
                        "msgtype": "markdown",
                        "markdown": {"content": segment_md},
                    }
                    resp = requests.post(webhook_url, json=payload, timeout=10)
                    spider.logger.info(
                        f"[WeChat] å·²å‘é€ç¬¬{idx}/{len(segments)}æ®µ: {resp.json()}"
                    )
                    if idx < len(segments):
                        time.sleep(0.5)
            else:
                # æ–‡ç« é•¿åº¦é€‚ä¸­ï¼Œç›´æ¥å‘é€
                payload = {
                    "msgtype": "markdown",
                    "markdown": {"content": full_content},
                }
                resp = requests.post(webhook_url, json=payload, timeout=10)
                spider.logger.info(f"[WeChat] å·²å‘é€æ–‡æœ¬æ¶ˆæ¯: {resp.json()}")

            # å‘é€å›¾ç‰‡
            images = article_data.get("images", [])
            if images:
                settings_obj = getattr(spider, "settings", None) or getattr(
                    getattr(spider, "crawler", None), "settings", None
                )
                images_store = (
                    settings_obj.get("IMAGES_STORE") if settings_obj else None
                )
                images_dir = (
                    Path(images_store)
                    if images_store
                    else (self.data_dir / "images")
                )

                for idx, img_rel_path in enumerate(images, 1):
                    img_path = images_dir / img_rel_path
                    if not img_path.exists():
                        continue
                    try:
                        with open(img_path, "rb") as f:
                            img_data = f.read()

                        b64 = base64.b64encode(img_data).decode("utf-8")
                        md5 = hashlib.md5(img_data).hexdigest()

                        img_payload = {
                            "msgtype": "image",
                            "image": {"base64": b64, "md5": md5},
                        }

                        max_retries = 3
                        for retry in range(max_retries):
                            img_resp = requests.post(
                                webhook_url, json=img_payload, timeout=10
                            )
                            resp_data = img_resp.json()
                            if (
                                resp_data.get("errcode") == 45009
                                and retry < max_retries - 1
                            ):
                                wait_time = 5 * (retry + 1)
                                spider.logger.warning(
                                    f"[WeChat] å›¾ç‰‡ {idx}/{len(images)} é¢‘ç‡è¶…é™ï¼Œç­‰å¾… {wait_time}s åé‡è¯• ({retry+1}/{max_retries})"
                                )
                                time.sleep(wait_time)
                            else:
                                spider.logger.info(
                                    f"[WeChat] å·²å‘é€å›¾ç‰‡ {idx}/{len(images)}: {img_rel_path} - {resp_data}"
                                )
                                break

                        if idx < len(images):
                            time.sleep(1)
                    except Exception as e:
                        spider.logger.error(
                            f"[WeChat] å‘é€å›¾ç‰‡å¤±è´¥ {img_rel_path}: {e}"
                        )
        except Exception as e:
            spider.logger.error(f"[WeChat] å‘é€å¤±è´¥: {e}")
